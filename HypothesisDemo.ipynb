{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:02.026866Z",
     "start_time": "2020-02-27T16:52:02.023694Z"
    }
   },
   "source": [
    "# Import Statements + Notebook Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:30.711480Z",
     "start_time": "2020-02-27T19:40:29.893988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Requirements:\n",
    "# pip install ipytest\n",
    "# pip install hypothesis\n",
    "# pip install spacy\n",
    "# Follow spacy instructions to download en_core_web_sm or just don't run the final example\n",
    "\n",
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "ipytest.config(rewrite_asserts=True, magics=True)\n",
    "__file__ = \"HypothesisDemo.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:30.724573Z",
     "start_time": "2020-02-27T19:40:30.713766Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some objects to do determinstic tokenziing of arbitrary texts\n",
    "\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import List\n",
    "\n",
    "Token = namedtuple(\"Token\", \"string span\")\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    name: str = \"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def tokenize(self, string: str) -> List[Token]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, string: str) -> List[Token]:\n",
    "        return self.tokenize(string)\n",
    "    \n",
    "class WhiteSpaceTokenizer(Tokenizer):\n",
    "    name = \"white_space_tokenizer\"\n",
    "    \n",
    "    def tokenize(self, string: str) -> List[Token]:\n",
    "        substrings = string.split()[::-1]\n",
    "        tokens = []\n",
    "        left_index = 0\n",
    "        while substrings:\n",
    "            substring = substrings.pop()\n",
    "            span_left = string.index(substring, left_index)\n",
    "            span_right = span_left + len(substring)\n",
    "            tokens.append(Token(substring, (span_left, span_right)))\n",
    "            left_index = span_right + 1\n",
    "        return tokens\n",
    "\n",
    "class AsciiCharacterTokenizer(Tokenizer):\n",
    "    name = \"ascii_character_tokenizer\"\n",
    "\n",
    "    def tokenize(self, string: str) -> List[Token]:\n",
    "        return [\n",
    "            Token(char, (i, i + 1))\n",
    "            for i, char in enumerate(self._string_to_ascii(string))\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _string_to_ascii(string: str) -> List[str]:\n",
    "        return [char if ord(char) < 128 else \"<NON-ASCII>\" for char in string]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.114330Z",
     "start_time": "2020-02-27T19:40:30.727060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...x                                                                                                                                                                                                                                                                                                     [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_white_space_tokenizer_returns_empty_string():\n",
    "    tokenizer = WhiteSpaceTokenizer()\n",
    "    assert tokenizer(\"\") == []\n",
    "\n",
    "def test_white_space_tokenizer_returns_expected_output():\n",
    "    tokenizer = WhiteSpaceTokenizer()\n",
    "    test_string_0 = \"♥O◘♦♥O◘♦ nae nae\"\n",
    "    assert tokenizer(test_string_0) == [\n",
    "        Token(\"♥O◘♦♥O◘♦\", (0, 8)),\n",
    "        Token(\"nae\", (9, 12)),\n",
    "        Token(\"nae\", (13, 16)),\n",
    "    ]\n",
    "\n",
    "def test_ascii_character_tokenizer_returns_expected_output():\n",
    "    tokenizer = AsciiCharacterTokenizer()\n",
    "    test_string_0 = \"♥O◘♦♥O◘♦ nae nae\"\n",
    "    assert tokenizer(test_string_0) == [Token(string='<NON-ASCII>', span=(0, 1)),\n",
    "                                        Token(string='O', span=(1, 2)),\n",
    "                                        Token(string='<NON-ASCII>', span=(2, 3)),\n",
    "                                        Token(string='<NON-ASCII>', span=(3, 4)),\n",
    "                                        Token(string='<NON-ASCII>', span=(4, 5)),\n",
    "                                        Token(string='O', span=(5, 6)),\n",
    "                                        Token(string='<NON-ASCII>', span=(6, 7)),\n",
    "                                        Token(string='<NON-ASCII>', span=(7, 8)),\n",
    "                                        Token(string=' ', span=(8, 9)),\n",
    "                                        Token(string='n', span=(9, 10)),\n",
    "                                        Token(string='a', span=(10, 11)),\n",
    "                                        Token(string='e', span=(11, 12)),\n",
    "                                        Token(string=' ', span=(12, 13)),\n",
    "                                        Token(string='n', span=(13, 14)),\n",
    "                                        Token(string='a', span=(14, 15)),\n",
    "                                        Token(string='e', span=(15, 16))\n",
    "                                       ]\n",
    "# This final test hasn't been written yet. Exercise: remove the decorator and write a \n",
    "# passing test.\n",
    "@pytest.mark.xfail\n",
    "def test_ascii_character_tokenizer_string_to_ascii():\n",
    "    tokenizer = AsciiCharacterTokenizer()\n",
    "    assert tokenizer._string_to_ascii(\"♥O◘♦♥O◘♦\") == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than likely, you just ran `tokenizer._string_to_ascii(\"♥O◘♦♥O◘♦\")` and then pasted the output into the test. That is a bit circular, isn't it? We're testing our code with \"expected output\" that is simply the result of our code being executed in the first place... \n",
    "\n",
    "We probably shouldn't have much faith in this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we change the code being tested?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.120725Z",
     "start_time": "2020-02-27T19:40:31.116323Z"
    }
   },
   "outputs": [],
   "source": [
    "class AsciiCharacterTokenizer(Tokenizer):\n",
    "    name = \"ascii_character_tokenizer\"\n",
    "\n",
    "    def tokenize(self, string: str) -> List[Token]:\n",
    "        return [\n",
    "            Token(char.lower(), (i, i + 1))\n",
    "            for i, char in enumerate(self._string_to_ascii(string))\n",
    "        ]\n",
    "    @staticmethod\n",
    "    def _string_to_ascii(string: str) -> List[str]:\n",
    "        return [char if ord(char) < 128 else \"<NON-ASCII>\" for char in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.328553Z",
     "start_time": "2020-02-27T19:40:31.122482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                                                                                                                                                                                                                                                        [100%]\n",
      "=================================================================================================================================================== FAILURES ===================================================================================================================================================\n",
      "____________________________________________________________________________________________________________________________ test_ascii_character_tokenizer_returns_expected_output ____________________________________________________________________________________________________________________________\n",
      "\n",
      "    def test_ascii_character_tokenizer_returns_expected_output():\n",
      "        tokenizer = AsciiCharacterTokenizer()\n",
      "        test_string_0 = \"♥O◘♦♥O◘♦ nae nae\"\n",
      ">       assert tokenizer(test_string_0) == [Token(string='<NON-ASCII>', span=(0, 1)),\n",
      "                                            Token(string='O', span=(1, 2)),\n",
      "                                            Token(string='<NON-ASCII>', span=(2, 3)),\n",
      "                                            Token(string='<NON-ASCII>', span=(3, 4)),\n",
      "                                            Token(string='<NON-ASCII>', span=(4, 5)),\n",
      "                                            Token(string='O', span=(5, 6)),\n",
      "                                            Token(string='<NON-ASCII>', span=(6, 7)),\n",
      "                                            Token(string='<NON-ASCII>', span=(7, 8)),\n",
      "                                            Token(string=' ', span=(8, 9)),\n",
      "                                            Token(string='n', span=(9, 10)),\n",
      "                                            Token(string='a', span=(10, 11)),\n",
      "                                            Token(string='e', span=(11, 12)),\n",
      "                                            Token(string=' ', span=(12, 13)),\n",
      "                                            Token(string='n', span=(13, 14)),\n",
      "                                            Token(string='a', span=(14, 15)),\n",
      "                                            Token(string='e', span=(15, 16))\n",
      "                                           ]\n",
      "E       AssertionError: assert [Token(string...=(5, 6)), ...] == [Token(string...=(5, 6)), ...]\n",
      "E         At index 0 diff: Token(string='<non-ascii>', span=(0, 1)) != Token(string='<NON-ASCII>', span=(0, 1))\n",
      "E         Full diff:\n",
      "E         - [Token(string='<non-ascii>', span=(0, 1)),\n",
      "E         ?                 ^^^ ^^^^^\n",
      "E         + [Token(string='<NON-ASCII>', span=(0, 1)),\n",
      "E         ?                 ^^^ ^^^^^\n",
      "E         -  Token(string='o', span=(1, 2)),...\n",
      "E         \n",
      "E         ...Full output truncated (36 lines hidden), use '-vv' to show\n",
      "\n",
      "<ipython-input-5-b7e961085b4c>:4: AssertionError\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_ascii_character_tokenizer_returns_expected_output():\n",
    "    tokenizer = AsciiCharacterTokenizer()\n",
    "    test_string_0 = \"♥O◘♦♥O◘♦ nae nae\"\n",
    "    assert tokenizer(test_string_0) == [Token(string='<NON-ASCII>', span=(0, 1)),\n",
    "                                        Token(string='O', span=(1, 2)),\n",
    "                                        Token(string='<NON-ASCII>', span=(2, 3)),\n",
    "                                        Token(string='<NON-ASCII>', span=(3, 4)),\n",
    "                                        Token(string='<NON-ASCII>', span=(4, 5)),\n",
    "                                        Token(string='O', span=(5, 6)),\n",
    "                                        Token(string='<NON-ASCII>', span=(6, 7)),\n",
    "                                        Token(string='<NON-ASCII>', span=(7, 8)),\n",
    "                                        Token(string=' ', span=(8, 9)),\n",
    "                                        Token(string='n', span=(9, 10)),\n",
    "                                        Token(string='a', span=(10, 11)),\n",
    "                                        Token(string='e', span=(11, 12)),\n",
    "                                        Token(string=' ', span=(12, 13)),\n",
    "                                        Token(string='n', span=(13, 14)),\n",
    "                                        Token(string='a', span=(14, 15)),\n",
    "                                        Token(string='e', span=(15, 16))\n",
    "                                       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a failing test because of a simple change and we need to manually change our test case to capture the change in source code. This is fine, but it feels like tests written this way aren't really helping us much.\n",
    "\n",
    "(Hint: they feel like they're not helpful because they're not really testing properties of our code, just arbitrary output.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we add a new object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.338699Z",
     "start_time": "2020-02-27T19:40:31.330409Z"
    }
   },
   "outputs": [],
   "source": [
    "class AsciiCharacterTokenizer(Tokenizer):\n",
    "    name = \"ascii_character_tokenizer\"\n",
    "\n",
    "    def tokenize(self, string: str) -> List[Token]:\n",
    "        return [\n",
    "            Token(char, (i, i + 1))\n",
    "            for i, char in enumerate(self._string_to_ascii(string))\n",
    "        ]\n",
    "\n",
    "class AsciiLowercaseCharacterTokenizer(Tokenizer):\n",
    "    name = \"ascii_lowercase_character_tokenizer\"\n",
    "\n",
    "    def tokenize(self, string: str) -> List[Token]:\n",
    "        return [\n",
    "            Token(char.lower(), (i, i + 1))\n",
    "            for i, char in enumerate(self._string_to_ascii(string))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.607646Z",
     "start_time": "2020-02-27T19:40:31.342793Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                                                                                                                                                                                                                                                        [100%]\n",
      "=================================================================================================================================================== FAILURES ===================================================================================================================================================\n",
      "_______________________________________________________________________________________________________________________ test_ascii_lowercase_character_tokenizer_returns_expected_output _______________________________________________________________________________________________________________________\n",
      "\n",
      "    def test_ascii_lowercase_character_tokenizer_returns_expected_output():\n",
      "        tokenizer = AsciiLowercaseCharacterTokenizer()\n",
      "        test_string_0 = \"♥O◘♦♥O◘♦ nae nae\"\n",
      ">       assert tokenizer(test_string_0) == [Token(string='<NON-ASCII>', span=(0, 1)),\n",
      "                                            Token(string='O', span=(1, 2)),\n",
      "                                            Token(string='<NON-ASCII>', span=(2, 3)),\n",
      "                                            Token(string='<NON-ASCII>', span=(3, 4)),\n",
      "                                            Token(string='<NON-ASCII>', span=(4, 5)),\n",
      "                                            Token(string='O', span=(5, 6)),\n",
      "                                            Token(string='<NON-ASCII>', span=(6, 7)),\n",
      "                                            Token(string='<NON-ASCII>', span=(7, 8)),\n",
      "                                            Token(string=' ', span=(8, 9)),\n",
      "                                            Token(string='n', span=(9, 10)),\n",
      "                                            Token(string='a', span=(10, 11)),\n",
      "                                            Token(string='e', span=(11, 12)),\n",
      "                                            Token(string=' ', span=(12, 13)),\n",
      "                                            Token(string='n', span=(13, 14)),\n",
      "                                            Token(string='a', span=(14, 15)),\n",
      "                                            Token(string='e', span=(15, 16))\n",
      "                                           ]\n",
      "\n",
      "<ipython-input-7-b2e8f25cf476>:4: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "<ipython-input-2-874929ffc6b4>:21: in __call__\n",
      "    return self.tokenize(string)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <__main__.AsciiLowercaseCharacterTokenizer object at 0x7fab808b1b38>, string = '♥O◘♦♥O◘♦ nae nae'\n",
      "\n",
      "    def tokenize(self, string: str) -> List[Token]:\n",
      "        return [\n",
      "            Token(char.lower(), (i, i + 1))\n",
      ">           for i, char in enumerate(self._string_to_ascii(string))\n",
      "        ]\n",
      "E       AttributeError: 'AsciiLowercaseCharacterTokenizer' object has no attribute '_string_to_ascii'\n",
      "\n",
      "<ipython-input-6-304f684efb2d>:16: AttributeError\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_ascii_lowercase_character_tokenizer_returns_expected_output():\n",
    "    tokenizer = AsciiLowercaseCharacterTokenizer()\n",
    "    test_string_0 = \"♥O◘♦♥O◘♦ nae nae\"\n",
    "    assert tokenizer(test_string_0) == [Token(string='<NON-ASCII>', span=(0, 1)),\n",
    "                                        Token(string='O', span=(1, 2)),\n",
    "                                        Token(string='<NON-ASCII>', span=(2, 3)),\n",
    "                                        Token(string='<NON-ASCII>', span=(3, 4)),\n",
    "                                        Token(string='<NON-ASCII>', span=(4, 5)),\n",
    "                                        Token(string='O', span=(5, 6)),\n",
    "                                        Token(string='<NON-ASCII>', span=(6, 7)),\n",
    "                                        Token(string='<NON-ASCII>', span=(7, 8)),\n",
    "                                        Token(string=' ', span=(8, 9)),\n",
    "                                        Token(string='n', span=(9, 10)),\n",
    "                                        Token(string='a', span=(10, 11)),\n",
    "                                        Token(string='e', span=(11, 12)),\n",
    "                                        Token(string=' ', span=(12, 13)),\n",
    "                                        Token(string='n', span=(13, 14)),\n",
    "                                        Token(string='a', span=(14, 15)),\n",
    "                                        Token(string='e', span=(15, 16))\n",
    "                                       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same problem as above. We tediously write new test cases for new objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# So"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**We've created some very simple objects and tested them with simple expected outputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Should we have faith in these tests? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "  - Well, we're testing with a few toy examples for which we already knew the correct answer.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### What if we change the behavior of these objects?\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:34:08.354514Z",
     "start_time": "2020-02-27T17:34:08.350057Z"
    },
    "hidden": true
   },
   "source": [
    "  - Because we're testing with very rigid \"expected outputs,\" every time we change something minor in our source code, we have to update our test cases. \n",
    "  - At that point, are our test cases even *meaningful*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### What if we add new objects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "  - We have to figure out what the expected output for these new objects and add tests for them accordingly.\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property-based Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instead of checking that functions and methods produce expected outputs, property-based tests check that they satisfy expected properties.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A property-based test is a formal way of asserting \"given some input, the output should have these observable properties.\" Therefore, property-based testing requires some *generators* of what inputs to functions may look like.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.624989Z",
     "start_time": "2020-02-27T19:40:31.612384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q5z;O\"gA7:+sX#;p{%8`w/g\\\\`g7Lz2%L~ [Y(`n+fc>{|$W! I26o3>%|TK7zmT*c_^-\\']n_m7HKOUjNc\\\\\"J*p+:l@nRo:v9N&/Y'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple generator for strings:\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_string(len: int = 1000):\n",
    "    characters = string.ascii_letters + \" \" + string.digits + string.punctuation\n",
    "    return \"\".join(random.choice(characters) for _ in range(len))\n",
    "\n",
    "generate_string(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.845491Z",
     "start_time": "2020-02-27T19:40:31.629484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                                                                                                                                                                                                        [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "# Here we have a test which uses a generated test case. It fails *some* of the time, but\n",
    "# usually it doesn't. This isn't good enough.\n",
    "def test_white_space_tokenizer_basic_properties():\n",
    "    tokenizer = WhiteSpaceTokenizer()\n",
    "    string = generate_string()\n",
    "    tokens = tokenizer(string)\n",
    "    # Now, what are some properties we might want to test?\n",
    "    \n",
    "#     # When we concatenate all of our tokens back together we should have a same length string\n",
    "    assert len(\" \".join([token.string for token in tokens])) == len(string)\n",
    "    \n",
    "#     # For that matter, it should be the exact same string\n",
    "    assert \" \".join([token.string for token in tokens]) == string\n",
    "    \n",
    "    # For each token, the span should be able to retrieve the original substring\n",
    "    for token in tokens:\n",
    "        assert string[token.span[0]:token.span[1]] == token.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem solved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:52:59.335499Z",
     "start_time": "2020-02-27T17:52:59.332357Z"
    }
   },
   "source": [
    "**No**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:53:48.412771Z",
     "start_time": "2020-02-27T17:53:48.409068Z"
    }
   },
   "source": [
    "**Hypothesis is a library that fundamentally does two things:**\n",
    "\n",
    "- It helps us write generators for data of different types\n",
    "\n",
    "- It applies those generators to our test cases and tracks the results\n",
    "\n",
    "\n",
    "https://hypothesis.readthedocs.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Generating Data With Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:57:03.035561Z",
     "start_time": "2020-02-27T17:57:03.033378Z"
    },
    "hidden": true
   },
   "source": [
    "## Basic Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.850134Z",
     "start_time": "2020-02-27T19:40:31.847486Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from hypothesis import strategies as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.855506Z",
     "start_time": "2020-02-27T19:40:31.851964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis has built in types\n",
    "text = st.text()\n",
    "integer = st.integers()\n",
    "date = st.datetimes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:31.862630Z",
     "start_time": "2020-02-27T19:40:31.857501Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothesis.searchstrategy.lazy.LazyStrategy\n"
     ]
    }
   ],
   "source": [
    "# Great! So now we have random examples of text, integers, and dates?\n",
    "print(type(text))\n",
    "assert type(text) == type(integer) == type(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:32.473628Z",
     "start_time": "2020-02-27T19:40:31.864290Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# These are customizable *strategies* for generating data:\n",
    "import datetime\n",
    "\n",
    "assert type(text.example()) == str\n",
    "assert type(integer.example()) == int\n",
    "assert type(date.example()) == datetime.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:32.502904Z",
     "start_time": "2020-02-27T19:40:32.475348Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1579964525\n",
      "104\n",
      "14220\n",
      "2023257522\n"
     ]
    }
   ],
   "source": [
    "positive_integers = st.integers(min_value=0)\n",
    "for _ in range(5):\n",
    "    print(positive_integers.example())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Types of Type with Type Other Than Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Typically, we want to write tests to check properties of objects that consume more complicated data. In our case, we might want a \"document\" data type that more closely resembles the type of text we might see.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:32.508135Z",
     "start_time": "2020-02-27T19:40:32.504589Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@st.composite # \"The composite decorator works by converting a function that returns one example into a function that returns a strategy that produces such examples\"\n",
    "def document(draw):\n",
    "    document = \"\"\n",
    "    for _ in range(draw(st.integers(1, 100))):\n",
    "        document += draw(st.text())\n",
    "        document += \" \"\n",
    "    return document.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:33.299048Z",
     "start_time": "2020-02-27T19:40:32.510281Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "򚿬\u0018 񹛯 \u001a 0𔨰 􅰡&*\u0017 񕠀񬫣🷭\u0018\"򧻞  󣱂򙍅\u001e",
      "\u0012  )# 򈣛󔸄    -򓎕\u001d",
      "𩸌\" 򁪳󹶍\u00000󔭑𽹭\u0010\u001c",
      " 񼙂򿽎\n",
      "\u0017.󭓵񤤶\u001d",
      "񰛶󯅥 􁕓\n",
      "䪖\u001f 𗰋򂫵\u001e",
      " \u0002󧷔𱢙򈔑񻢜򊜎\u0005򌰴 򯇐# 𨄶󒪲󸛁𿎡򁊗󁟌  񼤫\u001f \u000b",
      "(򪒗\u0011 \u0019𦚭,\u0016𬋤\u0017\u0012\u0013 \u0012\u0000\u0001\u0013𛨜\u000b",
      "\f",
      "$\u0004򛣭!\u0014\u0002\u0006)󐂀\u0007'\f",
      "񚛭 򣢨򉑄񖈴\u0003󤇃 𪲉𠦂񼚯\u0000󗜐󿒹  ƍ0\u000f󯴂񮋊 򢰓\u0003󩯇򅉉򣊉%\u0003󹥳򱥻$\u0017 $\n",
      "񴗊𠘴\u0001 \u001a񚜃\u0000 \u001d",
      "񖦫􃝔\u001b򫴔\u001b\u001b+\u0015%\u0005#򟍠\u0018򝞺\u0006\u0013凮 𲮂󰣁󈶳򂝐񞿒񱉟􆃸\u001d",
      "򗯓􌣀\u000e\u0006񥴃 \u0018򕁩\t󖮎\f",
      "𳁎񣱗󄓅𨝳􋖹􋙾􃓪\u0006#򼡡\u0005\f",
      "򀊫񀀕\u001c",
      "/   񡎁\u0014\u001f\u0002񄡾\n",
      "\u0007  \u0018 '\u0006\u0005+ !$ 򒶃񺗰  \u0005*\u0012𿐵 𯉞򎗳񸃆!\n",
      "񵓨󈟫\u0004󫖰𢗴 󚠟򅂝!\u0017  \u0007  񐿻\u001a򠊢𪏣 򄰥\u000e𷍎+ 򅽠򄫗 #\u0015󮩫\f",
      "񛙁  \u001b𠤂&\u0004\u0010\u000f𣍔\u0011\u001e",
      "컂\u001f񈕑\u001b𔁓\u0007 \u0011\n",
      "򆔷\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_strategy = document()\n",
    "\n",
    "for _ in range(5):\n",
    "    print(doc_strategy.example()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T18:09:39.630576Z",
     "start_time": "2020-02-27T18:09:39.628496Z"
    }
   },
   "source": [
    "# Writing tests with Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:33.303686Z",
     "start_time": "2020-02-27T19:40:33.300736Z"
    }
   },
   "outputs": [],
   "source": [
    "from hypothesis import given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:33.609316Z",
     "start_time": "2020-02-27T19:40:33.305816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x                                                                                                                                                                                                                                                                                                        [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "@pytest.mark.xfail\n",
    "@given(doc_strategy)\n",
    "def test_white_space_tokenizer_basic_properties(string):\n",
    "    tokenizer = WhiteSpaceTokenizer()\n",
    "    tokens = tokenizer(string)\n",
    "    # Now, what are some properties we might want to test?\n",
    "    \n",
    "    # When we concatenate all of our tokens back together we should have a same length string\n",
    "    assert len(\" \".join([token.string for token in tokens])) == len(string)\n",
    "    \n",
    "    # For that matter, it should be the exact same string\n",
    "    assert \" \".join([token.string for token in tokens]) == string\n",
    "    \n",
    "    # For each token, the span should be able to retrieve the original substring\n",
    "    for token in tokens:\n",
    "        assert string[token.span[0]:token.span[1]] == token.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:33.615423Z",
     "start_time": "2020-02-27T19:40:33.610823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's imagine we want this object to raise an exception if passed an empty string:\n",
    "\n",
    "class WhiteSpaceTokenizer(Tokenizer):\n",
    "    name = \"white_space_tokenizer\"\n",
    "\n",
    "    def tokenize(self, string: str) -> List[Token]:\n",
    "        if not string:\n",
    "            raise ValueError(\"Epstein didn't kill himself!\")\n",
    "        substrings = string.split()[::-1]\n",
    "        tokens = []\n",
    "        left_index = 0\n",
    "        while substrings:\n",
    "            substring = substrings.pop()\n",
    "            span_left = string.index(substring, left_index)\n",
    "            span_right = span_left + len(substring)\n",
    "            tokens.append(Token(substring, (span_left, span_right)))\n",
    "            left_index = span_right + 1\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:33.619163Z",
     "start_time": "2020-02-27T19:40:33.617009Z"
    }
   },
   "outputs": [],
   "source": [
    "from hypothesis import assume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:35.300621Z",
     "start_time": "2020-02-27T19:40:33.620721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.                                                                                                                                                                                                                                                                                                       [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "# Without assume keyword\n",
    "@pytest.mark.xfail\n",
    "@given(doc_strategy)\n",
    "def test_white_space_tokenizer_basic_properties1(string):\n",
    "    tokenizer = WhiteSpaceTokenizer()\n",
    "    tokens = tokenizer(string)\n",
    "    \n",
    "    # For each token, the span should be able to retrieve the original substring\n",
    "    for token in tokens:\n",
    "        assert string[token.span[0]:token.span[1]] == token.string\n",
    "\n",
    "# With assume keyword\n",
    "@given(doc_strategy)\n",
    "def test_white_space_tokenizer_basic_properties2(string):\n",
    "    assume(string)\n",
    "    tokenizer = WhiteSpaceTokenizer()\n",
    "    tokens = tokenizer(string)\n",
    "    \n",
    "    # For each token, the span should be able to retrieve the original substring\n",
    "    for token in tokens:\n",
    "        assert string[token.span[0]:token.span[1]] == token.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are properties that all tokenizers should have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:35.314949Z",
     "start_time": "2020-02-27T19:40:35.302452Z"
    }
   },
   "outputs": [],
   "source": [
    "@given(doc_strategy)\n",
    "def test_some_new_tokenizer_basic_properties(string):\n",
    "    tokenizer = SomeNewTokenizer()\n",
    "    tokens = tokenizer(string)\n",
    "    # Now, what are some properties we might want to test?\n",
    "    \n",
    "    # Tokens should be a list of \"Token\" named tuples\n",
    "    assert isinstance(tokens, List)\n",
    "    for token in tokens:\n",
    "        assert isinstance(token, Token)\n",
    "        assert isinstance(token.string, str)\n",
    "        \n",
    "    # Each token span should be two integers, the second greater than the first\n",
    "    for token in tokens:\n",
    "        assert isinstance(token.span[0], int)\n",
    "        assert isinstance(token.span[1], int)\n",
    "        assert token.span[1] > token.span[0]\n",
    "        assert token.span[0] >= 0\n",
    "        assert token.span[0] < len(string)\n",
    "        assert token.span[1] > 0\n",
    "        assert token.span[1] <= len(string)\n",
    "        \n",
    "    \n",
    "    # For each token, the span should be able to retrieve the original substring (maybe?)\n",
    "    for token in tokens:\n",
    "        assert string[token.span[0]:token.span[1]] == token.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can apply some of these same properties to a new tokenizer object, even if we don't know exactly what its expected output is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:35.624228Z",
     "start_time": "2020-02-27T19:40:35.319505Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "\n",
    "class SpacyTokenizer(Tokenizer):\n",
    "    spacy_model = \"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = spacy.load(self.spacy_model)\n",
    "\n",
    "class SpacyEnCoreWebSmTokenizer(SpacyTokenizer):\n",
    "    name = \"spacy_en_core_web_sm_tokenizer\"\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    "\n",
    "    def tokenize(self, string: str) -> List[Token]:\n",
    "        doc = self.nlp(string, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "        return [Token(token.text, (token.idx, token.idx + len(token))) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:35.628954Z",
     "start_time": "2020-02-27T19:40:35.626516Z"
    }
   },
   "outputs": [],
   "source": [
    "from hypothesis import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T19:40:38.213914Z",
     "start_time": "2020-02-27T19:40:35.630987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                                                                                                                                                                                                        [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "@given(doc_strategy)\n",
    "@settings(deadline=None, max_examples=5)\n",
    "def test_spacy_tokenizer_basic_properties(string):\n",
    "    tokenizer = SpacyEnCoreWebSmTokenizer()\n",
    "    tokens = tokenizer(string)\n",
    "    # Now, what are some properties we might want to test?\n",
    "    \n",
    "    # Tokens should be a list of \"Token\" named tuples\n",
    "    assert isinstance(tokens, List)\n",
    "    for token in tokens:\n",
    "        assert isinstance(token, Token)\n",
    "        assert isinstance(token.string, str)\n",
    "        \n",
    "    # Each token span should be two integers, the second greater than the first\n",
    "    for token in tokens:\n",
    "        assert isinstance(token.span[0], int)\n",
    "        assert isinstance(token.span[1], int)\n",
    "        assert token.span[1] > token.span[0]\n",
    "        assert token.span[0] >= 0\n",
    "        assert token.span[0] < len(string)\n",
    "        assert token.span[1] > 0\n",
    "        assert token.span[1] <= len(string)\n",
    "        \n",
    "    \n",
    "    # For each token, the span should be able to retrieve the original substring (maybe?)\n",
    "    for token in tokens:\n",
    "        assert string[token.span[0]:token.span[1]] == token.string\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on fuzzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be a strict definition of \"property-based testing\" out there somewhere. I don't care. I think fuzzing is property-based testing where you're just testing for the property \"this program doesn't crash.\" Or property-based testing involves some ideas from fuzzing and applies them to unit testing type contexts. Either way they're similar concepts. Although you can test for properties of functions and objects without generating random inputs, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scratch] *",
   "language": "python",
   "name": "conda-env-scratch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
